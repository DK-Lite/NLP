# HMM
전통적인 확률 모델 빼고는 전부다 Discriminative Models 이다.
조건부확률로 바꿔서 계산하는 방법이 Generative model
## Introduction
- 인공지능의 기본되는 Task는 Classification
- Preprocessing - Feature extration - Classification
### Generative(확률) vs Discriminative(신경망) models
- Joint probability 해결하기위해 P(X|Y)
- P(Y|X) = P(Y)P(X|Y) -> 데이터 수가 후자가 많기때문에 뒤집는다.
Discriminative 모델의 단점
 - x가에 따라 확률이 극단적으로 간다.
 - 맥시멈 엔트로피 모델
Generative 모델
 모든 확률을 다 알아야한다.
| | y = 0 | y = 1 | px |
|-|-------|-------|----|
|x = 1 | 1/2 | 0 | 1/2 |
|x = 2 | 1/4 | 1/4 | 1/ 2|
| py | 3/4 | 1/4 | |
- joint probability 을 아는거는 모든 확률을 다 아는것
X = { x1, x2, x3 }
p(X) = P(xi| Y)
back off model - tri -> bi -> uni
- 하지만 카운트하는 단위가 맞지 않기때문에 섞어서 쓰면 말이 안된다.
- 그래서 성능이 떨어질수 밖에없다.
- Discriminative 모델은 된다 weight 를 모델이 알아서 정해주기 때문에
## Markov Chain
Markov Assumption(앞에 단어만 영향을 준다고 생각하자)
## HMM
why hidden?
State = pos
Observation - textual
형태소분석 형태소의 Sequence 를 보고있는다.
단어의 시퀀스를 보고 POS 태깅을 맞춰라
state 가 숨겨져 있음
observation == state 가 같으면 visiable
## Three Fundamental Question for HMM
- Evaluation Problem
    - 당신에게 모델이 주어졌을떄 시퀀스가 주어졌을때 확률을 계산하세요.
    - 분류를 하는 문제.
    - 각각 히든마르코프모델을 만들어서 각각 비교해보는걸 말함
    - P(S|O) O를 가지고 S를 예측하는 문제인데 모든 것을 비교해서 가장 높은 확률을 출력
    - S는 이미 준비된 데이터
- Decoding Problem
    - 다이나믹 프로그래밍, 계산하는 방법들
    - 히든 마르코프 모델은 A b 하고 파이로 이루어진다.
    - t, o 으로 계산이된다.
    - 상태가 주어졌을때 Oservation나올 확률
    - 자연어처리는 이게 다
    - O 숨겨져있는 품서태그의 시퀀스 찾는거
    - 모든 패스에 대해서 다 계산해야하기 때문에 N^(T+1) 가 된다.
- Learning or estimation Problem
    - 자연어 처리는 레이블된 코피스가 있어야한다.
    - 레이블이 이미 다 되어있기때 문에 모든 확률을 다 계산할수있어서 확률적으로 다 해결할수있다.
    - 전치사가 1000번 나오면 명사가 900번이면 확률이 높게나온다.
    - 확률을 잘 다듬는다.
    - 이부분이 없어도 잘 돌아간다.
    - 음성인식은 레이블링을 할수 없다. 음성인식은 unsupervised, 그래서 Estimation하는 방법을 알아야함
- EM 알고리즘
모든 비지도 학습
Train 데이터의 레이블이 없으면 EM으로 구함
```python
for{
Expectation Step;
Maximizaer Step;
}
```
내가 클래스피케이션 하고 확률 구하고 클래스구하고 확률구하고를 반복
- E Step
    - 레이블을 다는 행위 결과 막 주면 확률을 뽑을수있다.
    -
- M Step
    - 확률을 뽑아서 새로운 모델을 만드는 과정
    - 분류를 할수 있는 모델에 적용
- Semi supervised learning
    - 적당한 레이블 수치를 주고 비지도 학습을 한다.
    - 최대한 아는 정보를 반영해서 비지도 학습의 성능 높이고자 한다.
확률 분포가 수렴할때까지
데이터 피처중에 데이터 값이 적으면
해당 피처에 0 또는 1로 몰리게 된다.
그래서 해당 피처가 나타나면 민감한 모델이 되기때문에
그것을 풀어줘야한다.
1, 0 제곱의 썸
0.5 0.5
**한쪽으로 치우쳐져있을때 제곱의 썸이 크기때문에 그것을 빼줌으로서 치우치지 않게 한다.**
아는정보에서 균등하게 넣어야 R값이 작아진다.
엔트로피는 값이 비슷할때 가장 값이 크다.
>조건부 확률을 계산할
기븐 피처가 작아서 카운트가 몇번 안되면 한쪽으로 확쏠리수 있다.
그것을 위해서 잘모르는 부분은 조금 뺴줘서 N분의 1로 나눠니깐 평향되지 않고 더 좋더라
## MEMM
Label bias problem
local probability 때문에 생김
## CRF
그래서 기존에 확률로 만든상태로 계산을 하는게 아니라
전체를 확률이 아닌 스코어로 만들어놓고 계산(더하기)을 다한뒤에 나누겠다.
**확률모델에서는 최고였음**