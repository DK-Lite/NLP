# 자연어 처리 소개 및 개요
인공지능이란 넓은 의미로서 사람이 잘하는것을 기계가 잘하도록 하는것이다.
이런 기계가 잘 학습 할 수 있도록 필요한 요소가 데이터인데 그러한 데이터는
이미지, 텍스트, ... 이 될것이다.
특히 데이터 중에 이미지는 Sementic 정보를 넣기 힘들다. 그래서 이미지를 텍스트로 바꿀수 있는데
이러한 방법은 사람들이 의미를 파악하기가 쉬워진다. 그렇기 때문에 동영상 또한 결국은 Image의 연속이라서 Text로의 변화가 필요하다.

## Introduction of NLP
우리 인류는 글이라는 매체가 등장고 기록이 가능해짐으로서 인류가 발전 할 수 있었다. 이러한 글을 컴퓨터가 알게 한다는것이 자연어처리인데 이 것은 융합학문이기때문에 논문을 쓰기가 어렵다. 또한 자연어처리는 현재의 인공지능의 난이도와 비슷하다.

## Levels of NLP
> 사람하고 똑같은 대화시스템을 만드는게 자연어처리의 끝이다.
사람의 기본적으로 처리하는 방식
1. Top-down
2. Bottom-Up (자연어처리)
    - "나는 학교에 갔다." 이 문장의 의미를 파악하려면
        - 뜻을 갖는 작은 단위 : 형태소(더 자르면 뜻이 깨짐)
- Morphology POS Tagger (형태소 분석)
    - 품사 까지 찾음 - 묶어서 한 문장의 의미를 찾아감
    - 신문기사는 문장을 자르기가 편하다( Pieared Marker 가 명확 )
    - 어절 : 띄어쓰기 단위 \
    - 음절 : 초성 중성 종성 단위가 모여서 만들어지면 (소리의 잘라지는 단위) \
    - 음소단위 : 초성, 중성, 종성(ㅎ, ㄱ, ㅏ, ㅓ, ...) - 성능이 좋아지는데 느려진다.
    - Segmentaion - POS -
    - 불규칙 단어 : 곱다 - 곱+ㄴ - 고운 ( ㅂ의 불규칙 )
    - 그래서 형태소 분석이 쉽지 않다.
> **mecmb** : 형태소 분석 오픈소스
- Syntax
    - 각각의 어절이 어떤 관계를 가지고 있는지 찾아야한다.
    - 문법적인 관계를 찾는게 Syntax anlaytics
    - 동사가 문장의 의미나 구조를 다 가지고있다.
    - 나는(주격) 학교에(대상격) 간다(동사)
    -
- Sementics( Segmantic Role Labeler) : 의미분석
- Pragmatics
    - 화형론 마을 어떻게 쓸것인가에 관한 이론
    - 존댓말을 쓸것인가? 내가 목적을 이루기 위해서 똑같은 말을 쓰더라도 패턴이 달라야한다.
- Discourse
    - 담화론
    - 앞에 무슨말을 했냐에 따라서 달라 질 수 있다.
    - 한문장의 의미가 거기 들어간 단어만 가지고 파악하기가 어렵다.
    - "밥 먹었니?, 네", "밥 안먹었니?, 네" 와 같이 의미가 다르다.
    - 대화시스템 만들때 중요하다.

## Phonetics & Phonology (음성학)
음성 : 시그널 \
언어 : 디지털

## Morphology
- 일본과 한국어가 구조가 같다.(교착어 : 무언가 결합이 되어서 궁극적인 의미를 가진다.) \
- 라틴 (굴절어)
- 명사면 조사가 붙고 형용사 동사는 어미가 붙는다.
- 한국어는 중의성(ambiguites)이 강하다.( (먹는)배, (타는)배, (사람)배 )
- 인공지능의 문제점을 해결하는 가장 기본적인 문제는 중의성을 해소하는 것(Classification)
- 이러한 중의성을 해소하기 위해서는 Contextual Information이 중요하다( 옆에 어떤 단어들이 있느냐 )
## Syntax
- 문자에서 어절간의 구조를 파악한다.
- 구(Phrase) - 몇개의 어절이 묶여서 한뜻을 가짐, 절(Clause) -
- 만약에 문법의 오류가 나오면 전체적인 구조가 오류난다.
    - 영어는 문법적인 할당을 위치로 하기떄문에 잘된다.
    - 한국어는 어순이 자유롭다.(교착어 특징) - 구구전법(트리) 안맞다.

## Sementics
- ACTION, ACTOR, OBJECT
- 명제(참, 거짓), Predicate(술어논어) - 많은 술어형태를 하나로 표현하겠다,
- ACTION( ACTOR, OBJECT)
- 인공지능에서 지식을 표현하는 방법은 Predicted Calculus - 가독성이 높다.
- 컴퓨터가 알수있는 표현해 보아라 하면 Predicate이다.
- Sementics 까지하면 한 문장을 파악하는게 완료

## Pragmatics
- I`am sorry 붙이는거

## Discourse
- 앞쪽에 나온 발화들을 가지고 지금 들어온 문장의 의미를 조금 더 정확하게 파악할 수 있어야 한다.
    SRL 의미분석 각 단어의 관계
    - NER : Named Entity Recognition (고유명사) \
    정보검색기 부문에서 QnA 시스템에 필요성에 의해 고유명사를 잘 찾는 일이 중요해짐 \
    - PLO : Person, Location, Organization, Time, Date \
    청와대(Location)에서 밥을 먹는다? 그렇기 때문에 **중의성해소** 문제가 중요해짐 \
    이 부분에서 Contextual Informaion 이 중요해짐

## Linguistic Rules
"Fed raises interest rates"
- 앞에서 형태소 분석의 결과가 잘못되면 뒤에 Sytax 분석에서 잘 못될 가능성이 있다.
- 긴 문장은 형태소가 20개 이상 나온다. 20개중에 1개가 틀리면 95% 정확도 있지만 이 것 하나 때문에 뒤에 전체 인식률이 문제가 발생하기 때문에 자연어처리가 어렵다.
- 그래서 자연어처리에서는 중간중간의 처리단계를 없애는게 이슈이다. (아래 문제들 해결과제)
    - 형태소 분석을 안하고 NER을 어떻게 할것인가?
    - SRL도 POS만 받고 할 수 있을까?
- 자연어처리의 가장 큰 자산은 **Labeled data**

## NLP
- NLU 언어 이해, NLG 대화시스템에 액션을 선택
- Seq2Seq
> SQuAD : QnA시스템 질문을 주고 지문안에 있는데 그걸로 답을 찾아라
- 기계번역이 중요한 Application이지만 잘 안되었던 이유는 성능이 낮아서이다.
- 구글

# Probability
- sample space : 나올수있는 가지수
- 확률은 데이터가 많을수록 정확하다

### Basic Properties
- 확률은 0~1 사이
- 모두 합하면 1
- 각각의 확률을 더하면 전체 확률이 된다.

### Joint and conditional Probability
- P(A,B) = A, B의 교집합
- P(A|B) = P(A,B) / p(B)
- Classcification -> B가 주어졌을때 A를 구해라
- 패턴인식 : P(Class|Example)

### Bayes Rule
- 조건부확률로 바로 계산하기 어려우면 뒤집어서 할수 있다.
- P(A|B) = p(B|A) * P(A) / P(B) :
    - P(A|B) = Posterior Prob(사후확률)
    - P(A) =  Prior Prob(사전 확률)
    - P(B|A) = Likelihood
    - P(B) = Evidence

### Independence

### **Chain Rule**
- Language Model
    - 이 문장은 제대로된 문장인지 아닌지를 Score로 내는 것을 랭귀지 모델이라고함
    - P(나는 학교에 간다) -> 제대로 된 문장이면 확률이 높게 나옴
    - 하지만 형태소의 Sequence 라서 Corpus 에서 똑같은 문장이 나오기 힘들다.
    - Count 를 형태소단위로 한다.
    - 여전히 그래도 B가 길기 때문에 앞에만 보는 마르코프 모델을 사용
    - HMM ( hidden Markorp Model ) : 한 시퀀스에 대한 확률을 계산

### The Golden Rule (of )
-  argmax P(B|A)P(A)

### Random Variables
- P(X) : x가 Event,
- pmf : 이산그래프에서 확률을 계산
- pdf : 연속적인 그래프에서 면적을 계산하기 위해서 적분을 사용하는것

### Expectation & Veriance
- 기대값 : 어떤값의 기댓값은 평균값

### Frequentist Statistics
- 내가 원하는 갯수 / 전체 갯수
    - Parametric(분포정보) 방법 - 통계모델에서 평균하고 분산을 알면 정규분포 그릴수 있다.
        - 내가 확률 계산하려고보니 비슷한 그룹이 있다 - 종모양을 가지더라
        - 각각 분포의 특성을 정리해둔거
        - z-score는 상위 몇 % 인지를 알수 있다
        - **즉, 내가 가지고 있는 데이터가 정규분포를 따른다면 데이터를 더이상 필요하지 않는다.**
    - Non-parametric (안씀) 방법
        - 어느 분포인지 모르겠고 데이터 많이 뽑아서 그 데이터에서 계산하자

### MLE (Maximum Likeihood Estimate)
- 과거의 데이터를 기반으로 미래를 예측하는게 확률이다.
- 데이터 분포가 충분하면 Parametric으로하고 그렇지않으면 Non-Parametric으로 한다.
- ML : 주어진 데이터에서 그 확률이 맞다고 가정하고 예측
-

### Essential Information Theory
- 엔트로피 : 불확실성
    - (Find Card Game)
    - 16개의 카드가 있을때 질문을 해서 맞춰라한다면 확률은 1/16이다.
    - 정보량을 어떻게 계산할것이냐?
        - 스무고개 하듯 물어본다.
        - 쉬운 문제 일수록 적은 질문으로 물어본다.
        - 어려운 문제 일수록 많이 질문하는데 그걸 정보량이라고한다.
    - 내가 주사위를 던지는데 나 어떤거라도 상관없음(정보가없을때)는 0
    - 짝수냐 홀수냐 보다 숫자가 나오는게 정보량이 많기때문에 엔트로피 값이 높다.
    - 엔트로피는 항상 양수가 나온다.
- Information theory of Shannon
    - 각각의 엔트로피에 확률을 계산 - 전체 엔트로피
- Entorypy

- **Mutual Information ( 상호 정보 )**
    - 두 단어의 관련성을 수식으로 표현하기 위해 사용