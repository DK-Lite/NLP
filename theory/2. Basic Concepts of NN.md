# Basic Concepts Of NN
- 히든 노드를 늘리는거보다 레이어를 더 쌓는게 저장되는 경우의 수가 더 많이 늘어난다.
- 3 * 10 * 1 = 30 , 3 * 5 * 5 * 1 = 75
- 그것을 학습시킬려고했지만 Vanishing gradient 문제가 발생
- 이것을 힌튼 교수님 팀이 해결
- 하지만 이것을 정확하게 학습시키기 위해서는 데이터의 양이 많아져야한다.
### Apple Tree
- Tensor : 벡터
- 그래프 연산
### Backpropagation
- Loss Function 으로 상위 레이어에서 에러를 계산
- 미분 한뒤 방향을 가지고 W를 업데이트
- 다음 레이어는 업데이트된 값들을 종합해서 새로운 에러를 계산
- 아웃풋에서 생성된 에러값이 w를 타고가서 합해진다.
- 왜냐하면 에러값이 반영되어하기때문에 ( W의 비율 정도를 가지고)
- 반복
#### MSE
- (y-yt)^2 / 2
- /2 는 미분하기 편하라고
- 절대값은 미분이 안되기 때문에 취하지 않는다.
#### GSD
- w 변화방법 - 미분
- lr - 핑퐁
- Optimazation
#### Activation Function
- sigmoid ( 아웃풋 Layer 에서 많이 씀)
- tanH
- 스무스한 이유 미분해야하기 때문에
- ReLU ( 딥뉴럴넷이 학습이 되게끔 만든 것, 0보다 작으면 0, 크면 X 그대로 전달)
- Scoring Functions ( SoftMax )
    - 아웃풋 Layer에서만 사용
    - 실제 정답이 1, 0, 0 형식을 주는게 좋은데
    - 확률적인 방법으로 생각해낸 것이 Cross Entropy 이다
    - Cross Entropy를 사용하기 위해 Output을 확률적으로 만들어줘야한다.
    - 그래서 모든 출력값을 다 더해서 각 Output을 나누어주면 확률이 나온다.
    - 하지만 다 더하는 과정에 분모가 0이 나올수 있어서 E의 제곱으로 계산된다.
> 벡터는 소문자를 쓴다. wx의 inner product는 스칼라값이다.
> 매트릭스 x 벡터의 아웃풋은 벡터이다. ( Wx : 매트릭스는 대문자 )