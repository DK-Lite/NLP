# 기계 번역
## BRMT(Rule-based Machine Translation)
- 기본적인 자연어처리를 해줘야한다.(형태소분석, 구문분석, 의미분석, ...)
- 번역은 규칙기반으로(각각의 패턴에 맞는 방식으로 규칙을 정함)
- Ex) 프랑스어 → 형태소분석(프) → 구문분석(프) → (의미분석(프) → 의미분석(영)) → 구문분석(영) → 형태소분석(영) → 영어
- 공통된 하나의 언어(가상)를 만들어주고 여러 언어로 변경한다가 기본적인 전제
- 삼각형방식은 모든 언어에 대해서 페어를 만들어줘야한다.( 비효율적인 시스템 )
언어학자들을 갈아서 규칙만 만들어내면 된다.
## SMT(Statistical Machine Translation)
- P(e|f) : f(구)가 주어졌을때 e(번역문장)가 나올 최대의 확률을 구하는 문제
- 베이지안 룰을 적용 → P(f|e)*P(e) (언어모델, 어떤단어가 생성된 확률값)
- I like you → 대용량 Corpus로부터 각 단어마다 Count,
- P(e) → (I → like 나올 확률) * (like → you 나올 확률)
- P(f|e) → 직접 구할수 없으니 문장대신에 단어로 자름 "I 가 프랑스어로 나올 확률" 등등 들의 곱
- P(e|f) → 단점 : 번역은 잘 될수 있어도 어순이 틀릴수 있음
확률값을 구하기 위해서 대용량의 학습 코퍼스가 필요하다.
Ex) 스페인어 → 영어에 대해서 여러 문장이 존재 (대략 100만 문장이 존재한다.)
- 단어대 단어가 아닌 구 대 구로 번역을 할수있는 번역을 확장한것이 Phrase-based MT
- 구대구(언속된 하나의 덩어리)
## NMT(Nearal Machine Translation)
딥러닝을 기계번역에 적용한 모델, 일반적인 출력이 분류라고 한다면
## 단어정렬
EM 알고리즘
## 디코드
- Unknown word문제 :Byte pair encoding 같은 subword model을 주로 사용
## 성능 측정
BLEU score - 참고용
## 번역 후처리
## 다국어 말뭉치 추출
병렬코퍼스를 모으는게 문제.
자동으로 구할수 없을까? 두개의 문서가 아주 비슷(html, 구조)할 경우 병렬 문장일 확률이 높다.
웹사이트 크롤링 → 텍스트 추출 → 문서정렬 → 문장정렬
> STRAND : 웹으로 부터 병렬 코퍼스를 추출하기 위한 도구 (Html 문서의 유사도만 본다)
## 텍스트의 표현방식
## Word2Vec
Word Embedding - CBOW, Skip-gram
## Recurrent NN Encoder-Decoder for SMT
## Input Feeding Approch
## Transformer: Self Attention
# BERT
positional Encodeing, positional 임베딩으로 학습하는 방법이 있다.
BERT 는 Tranformer 인코더와 비슷하다.
L12, H=768, Head = 12
학습은?
기준이 되는 단어에서 뒤에 문맥이 바뀌면 어텐션되는 단어가 달라진다. (상호참조)
버트는 학습은 어떻게 하느냐?
Tranformer 에서 인코더에서 때서 학습을 되는데
2가지 make LM, NSP
NSP : 바로 연달아서 등장하는것과 임의로 연결시켜서 학습을 자동으로 정답 코퍼스를 만들수 있다.
그것을 클래스피케이션 하는것
Masked LM : 두개의 문장을 넣어준뒤에 랜덤에서 마스킹 을한다. 전체에 15프로 마스
토익문제에서 가로 넣은 부분에 단어를 맞추는 방식으로 학습
대용량으로 학습을 해야 성능이 좋다.
싱글센텐스 분류 문제, NRC문제, 개체명 인식도 가능
번역에는 별로다
# OpenAIGPT
트랜스 포머 부분에서 디코더를 뗌
인코더가 없으니깐  원래 어텐션 부분을 떼버림,
이러고 보면 결국은 버트랑 비슷해진다.
차이는 마스크드가 붙은거 밖에 없다.
생성을 위해서 사용한다.
랭귀지 모델처럼 대용량 학습을 하고 버트처럼 다양한 Approch에 사용할수있다.
# XLnet, Robota
# Zipfs
대부분이 워드는 레어 워드이기때문에 다 넣을 필요없다
대략적으로 15000개 정도의 워드수만 주로 사용되는 워드만 넣어주면 된다.
사전의 크기가 너무 크면 메모리 양이 많이 커지기때문에 비효율 적이다.
그래서 사전의 크기는 무작정 늘리수 없다.
왜 그럼 사전의크기가 늘어나는 걸까?
- 하나의 단어가 여러의 단어로 사용할수있고
- 하나의 "하다"로 부터 파생되는게 많다
- 복합명사
- 사람 이름들
그렇다면 쪼개면 되지 않는가?
그런데 대학생선교회, 이것을 쪼개면 잘못된 데이터로 나누어질수있다.
그래서 복합명사를 분할하는것도 자연어처리중에 하나이다.
그래서 한국어의 경우 형태소 분석으로 통해서 나누어준다.
결국은 다 쪼개면 된다. 하+였+다 로 다 나누어도 되고
15000개의 안에 들어올까지 잘 잘라주면된다.
어떻게 자를것이냐?
초기에는
한국어는 음절단위로 자르고, 영어같은경우 캐릭터 두개씩 짜르자. 언어별로 휴리스틱을 줌
그래서 등장한것이
**Byte pair Encoding**
긴단어를 레어워드를 자르고 내가 알고있는 단어가 될때까지 자르자가 핵심아이디어
원래는 압축기술에서 사용되던 기술
사전을 학습을 한다.
사전에 있는 빈도수가 가장 높은 2개를 하나의 새로운 단어를 만들어나간다.
연달아 있는것들간의 프리퀀시들을 보면서 높은 애들이 붙는다.
그럼 등록되지 않은 단어에 대해서는 어떻게 사용할것이가?
새로운 단어가 들어오면 롱기스트 매칭을하면서 자른애들이 데이터에 발견될때까지 붙여가면서 찾아낸다.
그렇게하면 새로운 단어를 찾을수있다.
하지만 이렇게하면 원래의 단어가 여러개로 쪼개질수있다.
한국어의 경우 형태소 분석을 한뒤에 바이트페어 인코딩을 학습시키면 사전이 잘 학습된다.
- 분석전 - 관계, 자가
- 형태소 분석이후 - 관계자, 가
## Using Monolingual Data
SMT 에서는 Monolingual 코퍼스를 사용했는데.
NMT에서도 이것을 쓸수 있지 않을까? 라는 아이디어로부터 다양한 모델이 나옴
인코더 디코더에 어텐션을 쓴것에
monolingual data가 있을때는 인풋데이터가 없어서 디코더만 학습시킨다.
**Synthetic source 시험문제**
## Backtranslation
기존의 NMT 시스템은 그대로 인데 원래 하려는것의 역방향을 하나 만든다.
반대 방향의 NMT 시스템을 만들고 가짜 답이 나오면 그것을 원 시스템에 넣는다.
## Left 2 right / right 2 left reranking
앞에서 부터 번역을 시작하는게 최선은 아니니깐
타겟언어를 리버스 시킨 마스크를 정방향 반대방향
# Baysian Dropout
RNN일때 한번 드롭아웃을 만들어주면 동일한 시간대에 드랍아웃을 적용
트랜스포머에서는 사용하진 않는다.
# checkpoint 앙상블
여러개의 모델을 학습한다음에 여러개의 모델을 다 더해서 결과를 뽑는다.
랜덤 시드를 다르게 줘서 여러 모델을 학습시켜서 종합하는방법
checkpoint는 각각 이터레이션 마다 모델들을 가지고 저장해둔뒤 앙상블을 해도 성능이 좋더라
# Coverage
코퍼스에 없는 단어에 대해서 입력언어의 히딘스테이트의 어텐션 값들을 누적해보니 100이 넘어가는것이 생기는데
이 누적된 값을 잘이용하면 되지 않을까?
# 언슈퍼바이즈
- 한국어 입력을 넣고 출력도 한국어로 만든다. (노이즈를 섞는다)
- 영어로 입력을 넣고 영어로 출력 (노이즈를 섞는다)
- 한국어와 영어의 히든스테이트의 양성이 다르면 안되니깐 연관성을 가지게
- GAN 이라는 학습을 집어넣어서 양성이 비슷하게끔 학습시킨다
- 그럼 두개의 히든스테이트가 비슷해진다.
- 한국어 집어넣으면 영어가 나오게한다.