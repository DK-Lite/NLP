# 딥러닝 기반의 Sequential Labeling
- 인공지능의 꽃은 Classcification
- 자연어처리는 기본적으로 입력이 Sequence(순차적) 데이터(형태소)이다.
- 단어의 갯수가 유동적인 부분을 해결하기위해 RNN을 이용
> **Vanila Model:** 가장 기본적인 모델
- MLP안에서 Hidden State 는 어느정도의 인풋 값을 가지고있다. 그것을 다음 MLP에 인풋으로 같이 넣어준다.
자연어처리는 Contextual Information가 중요하다. 즉, 주위의 단어에 따라 현재의 단어가 다른 뜻이 될수도 있다라는 것인데(중의성 문제) 이러한 문제를 해결하기 위해 이전 입력을 반영하는 RNN이 해소해줄수 있다.
## Traditional Language Models
주어진 문장이 맞냐 틀리냐를 판단하려고 할 시 문장자체가 일반적이지 못함으로 맞냐 틀리냐를 판단하기가 매우 어렵다.(유일한 문장일 가능성이 크기 때문에 Count 수가 너무 적음)
이 문제를 해결하려고 각 단어+문장 형식으로 확률적인 부분을 조금 해소하였지만 이것 마저도 문제가 되었다.(문장이 길어지거나 해당하는 문장의 count 수가 적다.)
그래서 나온 방법이 아래의 Markov모델 이다.
> Markov Assumption:
- 앞에 단어 하나가 주어지고 다음 단어를 보는것을 Bi-gram
- Count(W1, W2) : W1, W2가 동시에 나온 가지수
- Back off Model - 일단 트라이그램의 확률을계산하는데 확률이 나쁘다면(카운트 값이 Threshold 값보다 작다면),tri-gram을 안쓰고 bi-gram을 쓰는 것이다. 이것 마저 확률이 낮으면 uni-gram, 이렇게 뒤로 찾아가는 모델
- r*P(Wt), a*P(W|W) + b*P(W|W,W) 3개의 가중치를 주고 합하여 쓰는 방법이 있다.
하지만 1억개의 단어를 Bi-gram으로 표시하려면 1억*1억인데 이것을 DB로 관리하려면 램이 부족한 문제가 생긴다. 이러한 부분을 사전을 가지고 있지 않아도 자동으로 N-gram을 가질수있는 모델이 있었으면 좋겠는데 그것이 RNN
### RNN 등장
- Weight들이 Backpropagation을 하면서  잘할수록 만들어준다.
- 다음 단어가 나오는 방식으로 RNN을 학습 시키는 방법은 전에 배웠던 Markov Model 과 비슷하다. Markov모델의 확률적인 부분(앞 뒤 단어의 출현횟수)으로 Bi-gram으로 구하던 부분을 RNN은 레이어, 즉 가중치를 가지고 학습이 가능할수 있게 된다.
- Cost Function : Cross Entropy를 사용
> Cross Entropy 내가 Prediction한 값과 Label 값을 크로스로 확률을 계산한다.
### The VGP for LM
> **Vanishing Gradient Problem:** Weight Update를 위해서 편미분을 하게되는데 Layer가 많은 딥러닝의 경우 체인룰에 의해서 0~1사이의 값이 지속적으로 곱해지게 된다. 이러한 과정이 지속되면 미분한 값이 소실(0으로 수렴)되는데 이 문제를 Vanishing Gradient Problem 이라 한다.
- RNN 또한 Time별로 진행되어 RNN셀이 생성되어있는데 단어수가 많아질수록 Layer가 많이 쌓여있는 효과가 나타나기 때문에 Vanishing Gradient Problem이 발생
- 앞쪽에서 넘어온 정보가 지금 정보를 똑같이 넘기는게 아니라 필터링 과정을 거치고
조절하여 넘기게 되면 길이가 길어지더라도 중요한 정보는 계속 전달 되게 할수 있는데 이 방법을 이용 한것이 LSTM이다
- **Vanishing Gradient Problem**을 해결하는 방법
    - Initialize를 잘한다.
    - ReLU 이용
    - LSTM은 ReLU를 이용한것이 아니라 Gate를 이용하여 처리
### Example Annotation
Sentiment 문제를 해결하려면 어떤 감정에 대해서 결과값이 하나가 나오는데
Sequential Label 문제에서는 도입 하기가 어렵다. 왜냐하면 모든 인풋에 대해서 출력을 가져야하는데 이것을 해결하는 것이 BIO Tag 이다.
> BIO tag : 문장이 시퀀셜하게 쭉들어오면 태그를 붙이려면 모든 인풋에 대해서 아웃풋이 나와야하는데.
BI : 내가 관심이 있는 부분 ( B : Begin, I :나머지 )
O : 내가 관심이 없는부분
### Bidirectional RNNs
forward, backward
사람이 말을 할때도 forward로 말하기때문에 forward가 중요
하지만 backward도 중요,( 한국어는 동사가 뒤에 있기 때문에 특히나 더 중요)
- forward하는 RNN 과 Backward 하는 RNN을 두개를 생성한뒤 두개의 Hidden State를 concat 한뒤 아웃풋 레이어를 거쳐서 출력
> Stacking : Layer를 쌓는것을 의미
### 평가 방법
정확도를 계산하는방법은 정답인 데이터를 맞추었을경우 정확도는 오르게 된다. 하지만 데이터의 분포도가 Positive데이터가 적고 Negative 데이터가 많은 환경에서 Accuracy가 높다라는것은 좋은 모델이 아니다 왜냐하면 Positive의 데이터가 적기 때문에 여러 환경에 의해 테스트 되었다고 볼수 없다
이러한 이슈를 해결하는데에는 정확도를 계산하는 것이 아닌 데이터의 분포도를 포함하여 판단할수 있는 아래의 방법이 사용된다.
True & False : 실제 데이터 결과
Positive & Negative : 컴퓨터가 예측한 결과
- 정밀도(precision) : 컴퓨터가 True라 한것중에 실제 True의 비율
- 재현율(recall) : 실제 True인 것들중에 컴퓨터가 True라 한것의 비율
- P1 : 2 * (Precision * Recall) / (Precision + Recall) - 조화평균
> 조화평균 : 어떤 일정한 값에 관련해서 자료의 값이 계속해서 변화할때, 조화평균을 택하는것이 적당 Ex) 거리D를 속도X로 움직이고, 다시 거리 D를 속도 Y로 움직이면 2D를 움직인 평균 속도는 조화평균으로 구하는것이 올바름
## GRU(Gated Recurrent Units)
RNN구조의 단점은 느리다.(이전 Hidden 값이 나와야 다음 셀이 동작할수 있기때문에 분산처리가 안된다) *그래서 Transformer 모델이 나옴
- Gate : 두개의 정보를 어떤 것을 크게해서 다음으로 넘길것이냐
    - 얼마만큼 통과시키는것이기 때문에 Gate는 무조건 sigmoid 를 거쳐야한다.
- Elementwise operation : 같은 차원끼리 곱해서 똑같은 차원을 만들어냄
- Gate를 써서 hidden을 다르게 쓰겠다.
- GRU는 reset, update gate가 존재
- LSTM은 forget, input, output가 존재
    - Cell state 라는것이 하나더 있는데 정보를 잘 넘기기 위한 고속도로 ( 정보를 넘기는거에만 집중 )
- 대부분의 Bidiectional LSTM, GRU는 성능이 올라간다.
## **Attention Mechnism**
Seq-to-Seq 모델
Encoding, Decoding
단점 : Encode에서 Hidden 이 여러개 있는데 Last Hidden만 사용하는게 이상하다.
가장 멀리 있는 단어를 Decode 에서 제일 먼저 찾아야함
Decoder 특성상 앞에 부분이 틀리면 뒤에 엉망진창이 된다.
예시) "나는 너를 사랑해 -> I love you" 에서 "I"는 "나는" 이라는 단어와 거리가 멀다
이부분을 해결하기위해 한국어를 뒤로 뒤집어서 테스트를 수행하니 인식률이 개선되었다.
> parrello corpus : 번역처럼 쌍으로 존재하는 데이터
### Attention 역사
- 문장 Embbeding을 할 떄 모든 Hidden을 다 써보자에서 시작
- 다 써보는데 자동으로 할 수 있는 방법을 생각
- Decode hidden 전에 Encode hidden들과 Decode hidden 의 유사도(dot product)를 구해보자
- 나온 값들을 Softmax
- 그 4개의 값을 Weight Sum해서 Context Vector를 만든다.
- 그래서 아웃풋 쪽에 Layer가 하나 더 존재
- inner product는 학습이 안되는방법이라서 가운데다가 matrix를 넣어서 곱하자도 나옴
> Attention을 사용하면 VGP 현상을 해결할 수 있다.( 출력에 전달되는 오류를 각 Hidden 들에게 전달하기 때문에)
### Badanau vs Luongs Attention
Badanau 유사도를 계산 했을때 앞에서 나온 Hidden(attention후 나온 Context V)을 Decoder의 다음 출력에 사용하는데 이 부분이 이상하다.
루엉은 layer를 만들어서 출력하겠다.
일반적으로 루엉을 사용
# Attention Is All You Need
- Transformer는 인코딩은 빠른데 실제 인식은 빠르지 않다. (Encoding은 분산처리가 가능하지만 Decoding은 분산처리가 불가하다.)
- Decoding 하는 부분을  이제까지 생성하였던 단어들만 가지고 하는것(Masked Attetion)
> 학습을 할떄는 문장이 다 있는데
- Pivot 되는 부분이 있으면 Encoder - Decoder Attention
- Self-Attention은 Pivot이 없음
- 각각의 단어들간의 유사도가 높을수록 Topical Word일 확률이 높다.
- maked Attention앞에 단어 밖에 없기때문에 앞에 단어만 가지고 어텐션을 구한다.
"Encoder Self Attention, Encoder/Decoder Attention, Masked Decoder SelfAttention" 이 3가지 Attention 기법이 존재
### Positional Encoding
일반적인 Attention 은 위치정보가 자동으로 들어간다. 쌓이는 정보의 수준이 다름
하지만 Self Attention은 전체를 보기때문에 위치정보가 들어가지 않는다. 그래서 인위적으로 위치정보를 넣자고 한것이 Positinal Encoding!
- (첫번째 나온 애들은 일정한 정보를 추가로 주겠다.)
- 위치 Embddeing 값을 만들어냄
- Elementwise Sum을 함
- 단어수만큼의 주기를 가지고 word Embbeding 크기만큼 그래프를 만들어냄
- Ex) Word Embedding Size : 10, Sequence Len : 100
- 주기의 길이가 100이고 각 초기값이 다른 10개의 그래프를 생성
- 각 주기 위치다 10개의 값을 빼와서 Embedding Vector에 더한다.
### Residual Connection
딥하게 Layer를 쌓다보면 인풋의 정보가 사라지고 점점 지나가는데
이 인풋 정보를 살리기 위해서 hidden 값에 다가 input 값을 더해서 input을 올려준다.
> 정규화(normalize) : 값을 비슷하게 만듦 치우치지 않게
### Multi head Attention
- Scaled dot-product attention
- Q, K, V
    - Q(Query) : pivot 이 되는 벡터
    - K(Key) : 유사도 계산할때 나오는 Hidden 값
    - V(Value) : 나온 유사도와 곱해질 Hidden 값
> Attention 공식 : Softmax(Q*Kt/d) * V
- Encoder/Decoder 에서는 KV가 같다.
- Self-Attention에서는 QKV가 같다.
- 8개의 Multi Header로 나눔
- 512 / 8 = 64  NN를 거쳐서 64개로 만듦 (Q K V) 그래서 총 24개의 NN
- QKV 3개로 1개의 Context Vector가 8개 만들어지는데 그 8개를 concatate 시키면 원래 512차원이 만들어진다.
    - 그래서 Convolution NN 의 Filter의 역할을 하는것과 같다.
디코더에서는 한번에 다 보여주는것이 아니라
Y1,  Y1|Y2, Y1|Y2|Y3 으로 봐야한다.
## Hierachical Attention Networks for Document Classification
문서가 들어오면 그 문서의 주제를 찾음