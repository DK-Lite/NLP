# Conversation Modeling
Breif History of Dialogue Sysyems
딥러닝이 나오고 멀티 도메인을 위해서 챗봇을 만들수있게됨
APP Bot()
App 을 없애고 언어모델로 모든걸 컨트롤하는것이 Bot
운전중, 요리중, 손을 자유롭게 사용하지 못하는 환경에서는 봇으로 많이 갈것이다.
CUI 멀티도메인에 대한 부담감이 없음
GUI는 각 도메인마다 App을 실행해야함
## Challenges
- Variablity in Natural language
- Robustness
- Recall/Precision Trade-off ( CUI 가 정확하게 답변을 줘야하는데 그렇지 않은 환경이 많다. )
- Meaning Representaion ( 똑같은 말을 해도 다른 의미로 해석 됨)
- Common Sense, World Knowledge (세계 최고의 부자? 당연한 상식들을 잘 모를수가 있다.)
- Ability to Learn ( 자기학습이 필요한데 얘는 온라인 러닝이 안됨, 전체를 다시학습)
- Transparency (Ambiguity)
> Ambiguity : 애매한, 모호한
## Two Branches of Bots
대화를 할수 있는 모든 시스템을 챗봇이라고 함
- Task(Goal)-Oriented Bot(식당예약, 기차표예약)
    - 사용자와 봇이 이루고자 하는 골이 존재
- Chit-Chat Bot(대화, 튜링테스트)
    - 말을 이끌어가는것, 목적이 없고 자연스럽게 사람처럼 얘기하는것
### Task-Oriented DS
사람이 말을하고 Speech Recognition 이후
- Language Understanding(NLU)
    - Domain Identification (지금 들어온 말이 어떤 도메인에 해당하는지 찾는것, 멀티도메인을 위해서)
    - User Intent Detection (사용자가 어떤 의도로 말을 했는가."너무 덮네(request)", yes or no?, response?)
    - Slot Filling (날짜=, 장르=, 위치=, 좌석= , 유저에게서 얻어야하는 정보들)
저 말에 대한 의미를 분석했다가 NLU의 역할
- Dialogue Management
    - Dialogue State Tracking(DST) (내가 이루고자하는 골에 가기위해 어느 단계에 와있는가?)
    - Dialogue Policy Optimization(위 상태에 대해서 내가 해야한 Response 를 결정, request_location)
- Natural Language (Where are you located) - 실제 사용자에게 보여질 Response 만들어냄
#### Domain Identification
1. 타이완에서 먹을 음식 찾아줘
2. 레스토랑 DB, Taxi DB, Movie DB 를 선택하는 Classification
#### Intent Detection (Classification)
1. "**find** a good **eating place** for taiwanese food"
미리 응답하기 위한 영역들을 만들어놔야한다. (범주가 많다.)
2. FIND_RESTAURANT, FIND_PRICE, FIND_TYPE
- Domain, Intent 차이는 Domain 은 문맥을 안보고 문장만 보고 판단하는 경우가 많다. Intent는 앞에 문장이랑 전문장을 봐야 판단 할수있다.(의도파악)
    - Domain에서는 한문장에 대해서 Classification 시도
    - Intent는 문맥이 줄줄히 들어오고 의도를 파악
- NLP 하는 사람의 입장 ( Intention이 아닌 Dialogue Act)
    - Speech act 화행과 Predicator(서술자)의 조합으로 이루어짐
    - 화행을 분석하다하면 domain에 따라 분석할 필요가 없음
    - Domain Dependent 한 즉 서술어의 메인 Categori 가 Predicatior
        - 이번주말에 액션주말있니?(Speech act) - Request  -> find movie(predicator)
사용자의 의도가 화행과 서술자로만 이루어질수있는가? 감정이라는 요소에 따라 답이 다를수가 있음
"좀 떨리더라" -> 감정이 Love : 우와 부럽다. Fear : 괜찮아 잘했어
대화시스템의 문제의 답은 워드 임베딩에 있다. 복잡하게 모델을 구현할 필요가 없다.
> 발화-Uttrance-문장
문장이 들어오면 각 감정, 화행, 서술자 각 영향을 미치는 네트워크만 학습을 각자 시도한다.
* **Sentence Representation 이 가장 중요하다.**
#### Slot Filing
- "find a **good** eating place for **taiwanese** food"
- BIO 태깅으로 찾아냄(개체명인식이랑 같음) - Sengmentation 정보
    - B - type, I - type, O
    - B - type, I, O  -> 해당 방법이 더 좋음
- 주변 문맥을 더 많이 방영 하려고 노력해야한다.
- CRFs : 전체 문맥을 다시 보려고
> Bidirectional + LSTM(GRU) + CRF
- layer를 쌓을수록 추상적인 표현으로 된다. 하지만 어휘적인 정보가 필요할수도 있다.
    - 서울, 부산 * layer = 도시
    - 어휘적인 정보를 손실할수 있기때문에 Residual Connection 을 추가해준다.
- Seq2Seq 방법을 사용하는 이유
    - LSTM만으로는 문장 전체의 의미를 파악할수 없다,
    - 문장 전체의 의미를 봐야하기때문에 ( seq2seq 는 문장 전체 의미를 볼수있다.)
#### Distant Superivsion
개체명인식이 부착된 데이터를 구하기가 힘들다.
개체명 담고있는 사전은 쉽게 구할수 있다.
사전에 있는 문장에 다가 String 매칭(강제로 삽입)
패턴 매칭으로 그냥 막 붙이는것을 Distant Supervision
이 것의 핵심은 노이즈를 제거하는데 휴리스틱하게
- Bagging-Based **Active Learning**
100만개 코퍼스를 만듬 - Bagging - 복원추출을함
10만개씩 10개를 만듬 -> 말뭉치를 쪼갬 -> 모델을 만듬 -> 100만개 짜리를 태깅
-> 결과를 비교해서 똑같으면 믿어도 됨 -> 모두다 틀리면 문제가 있는거 -> 사람이 고쳐서 반복
레이블된 데이터가 없어도 성능을 올릴수 있다.(영역이 정해진뒤 몇번만 손봐주면된다)
### Dialogue
#### State Tracking
**내가 할일을 결정하기 위해 유저가 어떤 상태인지를 파악해야한다.**
레스토랑을 예약하는 다이얼로그다. 로봇이 가지고 있어야 하는 도메인 날리지가 있어야한다.
쉬운 방법은 프레임이다. (사용자에게 물어야 하는 프레임)
tpye, waiting, time, location 등등 다 채우는것이 우리의 목적인데
문장에 따라 여러개를 채울수 있는데..
이 모든 state 중에서 어디 위치에 있는지를 파악하는것이 State Tracking
- Slot filing 에서 끝난것 아닌가?
    - 실제 Task는 음성으로 들어온것 이기 때문에 확률로서 state를 결정한다.
- slot 후보들을 뽑아두고 확률 Table을 참조하여 유저에게 응답한다.
- 지금 한말이 어떤 Slot을 채우고 어떤 상태로 가있어야하는지? 질문을 해야할지?
#### Dialogue Policy
유저의 요청에 대해서 어떻게 답변을 할지를 정책을 구성
" 정답을 알려줄것인가?, 질문을 할것인가?, 확인을 해볼것인가? " 등
- Reinforcement Learning을 많이 사용
    - 시스템에게 바로 답을 주는게 아니라 잘했다 못했다를 전달
    - 대화를 하다가 맨끝으로왔는데 그 마지막 답변이 잘끝났다면 학습
    - Table -> Deep NN 으로 한것이 DQN
    - 유저의 State -> Action이 나옴 ->
    - Supervicesed 러닝을 하면 지정된 sequence 대로만 답변을 하지만 Reinforment는 다양항 답변이 나올수있다.
    - Supervised 된 네트워크위에 다르게 학습된 Reinforcement 네트워크를 씌운다.
    - Sample 이 다 안들어올수있는 Chatbot 환경에서 전략이 잘못 세워질수있음.(Correlation Problem in DQN)
    - 랜덤하게 골라서 다시 만들어서 그걸가지고 학습(시나리오대로 따라가지 않도록 하기위함)
- Reward?
    - Social Chatbots : 대화를 많이 해줄수록 Reward가 높아짐
    - Task-O-Bot : 짧게 질문하는게 좋은것이기때문에 Reward를 높게줌
    - 실제 유저가 주는게 가장 좋은데... 그래서 Simulated User(Supervised Learning으로 만듦)를 만들어서 Reward를 주게한다.
- End to End Dialogue System
입력과 출력을 주면 그냥 너가 알아서 학습해(최신 방향)
#### Natural Language Generation
들어온 분석을 답변으로 만들어내는 과정
- Function의 슬랏값에 따라 답변을 달리함
- 문장이 정형화된 자연스럽지 않은 형태로 나감
현재는 Language 모델을 만들어서 제너럴하게 답변하게 만듦
- 생성해야 하는 단어가 나오는게 아니라 중간에 있는 Slot들을 생성해냄(Frame 생성)
- Language 모델 학습하듯이... 기계번역에서 Decoder에서 하듯이 학습 다음단어 예측
- 봇에 인텐션을 넣고 타겟 Sentence를 생성
## Chit-Chat Bot
### 검색 기반 챗봇
- Sentence 와 Sentence 비교를 통해서 답변을 검색
- DB에 QnA 데이터가 담겨져 있는데 user의 query에 대해서 answer를 내보냄
- QQ 매칭, 매칭 된것을 내보낼지 안내보낼지 Classify를 하나 만들어야함
    - 색인텀을 기존 문서 검색처럼 하면 안된다. ( 명사)
    - 짜장면을 **먹고싶다**, 짜장면은 **먹을것이다**는 다르다. 그래서 보조 용언을 색인해야함
    - 양상, 시제 사전을 통해서 색인을 명시
- 그리고 검색 알고리즘으로 검색
- 매칭 된 놈이 쓸건지 안할건지 Classify 모델이 필요함, 모델 끝의 Score로 판단하면안됨
### 생성 기반 챗봇
가지고 있는 코퍼스외로는 답변을 할수 없다.
새로운 것이 들어와도 적당히 말을 만들어야 하지 않겠나?
- 딥뉴럴넷의 언어모델을 사용
- 단점은 컨트롤이 안됨(가끔 욕을 함)
- 머신 트렌스레이션이랑 비슷 ( Source, target 에 따라 답변을 만들어냄 )
- RNN, LSTM, Seq2Seq, Attention
> Beam Search : 각 생성 layer 에서 최상위 두개씩 골라가면서 높은 패스를 찾아감
> 형태소+음절 In / 형태소+음절 out 방식을 선호
## 챗봇 학습의 문제점
- 대화 말뭉치 수집의 어려움
    - 영어는 1000만쌍 이상 사용, 한국은 80만쌍...
    -
- 학습데이터가 충분치 않은 경우 발생하는 문제점을 해결하는 방법
    - Auto Encoder 처럼 똑같은 말을 반복하도록 언어학습을 시킴, 비 문법적인 말들을 잘 안하게 됨
    - 그다음 언어를 알게되니깐 다이얼로그를 학습
> 음절단위는 디코더가 길어지기 때문에 일반적인 말로 학습한다.(I dont know 문제, 짧은것을 선호하여지게 학습)
>
## 멀티-턴 챗봇
사용자의 발화 뿐만 아니라 대화 문맥을 고려하여 응답을 생성
## Byte paire Encodeing
BERT word Pieace 모델과 동일
Ex) 7개가 나올떄까지 반복
1. 한 문장에서 2개씩 단어를 묶는다.
2. 거기 묶어진 단어에서 각 단어의 전체 카운트를 계산
3.
자주 나오는 단어는 묶인다.
고유명사들은 대부분 잘린다.
Voca 사이즈가 많이 줄어든다. 형태소 분석기도 필요없어짐
말뭉치만 넣어서 지가 만들어낸다.
## Chatbot with persona
Persona : 약간 똘끼 있ㄴ...
사람의 성향에 따라 다른 답변을 원할때, 같은 질문이라도 다른 느낌으로 답변할수있다.
Persona에 대한 정보를 Decoder에 넣어준다. 각각 인물에 대한 내용을 인풋으로 넣어주고 답변을 각기 만들어낸다.
## 기계독해
## ConvAI3
우리가 대화를 하다가 챗봇이 새로운 정보가 필요하면 검색을 해서 날리지를 쥐어주면 날리지를 바탕으로 대답을 할수있는가?
지식의 바탕으로 다른 말을 만들어냄
Wizard of Wikipedia Task
1365개의 주제에 대해서 트레이닝
Knowledge-Powerd Chatbot
- knowledge selection : 질문에 대한 답변을 위한 웹에서 정보를 가져와서 그것을 Vector화