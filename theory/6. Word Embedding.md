# Word Embedding
자연어 처리의 인풋이 무엇이냐
문장, 단락, 단어, 문서
- 뉴럴넷은 인풋이 한번 정해지면 바뀌기가 어렵다.
- 의미있는 인풋을 만들기위해 LM 기본으로 만들자
- Word Embedding이 LM의 모델을 잘 나타내기때문에 End2End 시스템이 가능하다
### Pretraining
- 여러 시스템의 각각의 테스트데이터가 있는데 각 시스템을 위한 LM이 있는것은 이상하다.
- LM은 전체문장이 문법적으로 맞게 Scoring 을 할수있는 것, (Chain rule, markov model)
- P(w|W) 이 확률을 잘 Estimation 하는것이 LM
- 시스템의 분야와 상관없이 왕창 모으는것,
- LM은 unsupervise 러닝이 가능 ( 신문기사가 잘쓰여진 문서라 가정하면 단어간의 연관성을 찾을수있음)
- 각각의 실제 task 를 하기전에 미리 학습한다.
- task의 이 모델을 가져가서 쓴다
### Fine tuning
세세하게 조정한다.
- Task에 따라 트레이닝 데이터가 다를수 있다.
- Backpropagation할때 워드 임베딩까지 업데이트 할수있다.
## old NN 의 문제점
Initialization에 따라 Local minima 가 발생
## Learing Word Representation for NLP
Contextual information이 이용해서 학습한다.
## Approaches for Word Embedding
- NNLM
- Ranking Based
: 하나의 스코어가 나오게
- Skip-gram
input 과 아웃풋이 바뀜
가운대 단어를 주고 주변단어를 맞춰라
> 확률을 Log로 취하는 이유는 확률은 0~1사이 값인데 단어가 1만개가 되면 곱연산하면서 값이 적어지고 언더플로우가 난다.
그래서 곱이 되는 연산이 있으면 log로 바꾼다. > 음수가 나오기때문에 수식에 - 붙이게 된다.
** Cross Entropy **
중심 단어 주위에 단어들의 한꺼번에 loss를 구해서 학습을 시킨다.
데이터가 다양하기 때문에 위처럼 한번에 단어를 학습시켜도 된다.
하지만 코딩에서는 단어 두개의 loss를 구하면서 minibatch학습시키기 때문에 비슷하다.
> Dropout : 노드 중심, 히든에 있는 노드를 랜덤하게 꺼버린다.
> DropConnect : 노드는 그대로 있는데 엣지들을 꺼버린다.
아키텍처가 바뀌는것, 데이터가 많아지는것 같은 효과가 나옴, 수렴이 느려짐,
- CBOW
주변단어를 주고 가운대 단어를 맞춰라
- Glove, Fast-text
Static 한번 딱 만들면 쓰는거, 룩업테이블이 필요했음,
Dynamic 학습을 해 놨는대요 파인튜닝이 중심이 된다. , 모델을 들고가서 그냥써라
- ELMo - NER
- BERT
Task #1 : Masked LM
문장의 임의의 단어를 mask씌우고 그것을 맞추는 방향을 학습(마스크에도 룰이있음)
Task #2 : Nest Sentence Prediction
CLS 가 두 문장의 연속을 잘 맞추게끔 학습