# 문서요약
딥러닝이 나오기 전에는 Text를 생성해서 문서 요약하는 방법이 어려웠다.
ES 가 딥러닝
## Text Summarization
주어진 문서(1 or 여러개)에 대해서 주요한 내용을 간략하게 요약,
- Single-Document Summarization
- Mulit-Document Summarization
    - 특정한 주제가 주어짐
- Ouput
    - Extractive : 중요한 문장만을 추출
    - Abstractive : 새로운 요약문을 생성
- Machine learning methods
입력이 주로 한 문장이 아닌 문서가 주어진다.
긴 문장을 어떻게 처리할 것이냐? 문서 요약문을 만들때 어텐션이 잘못되서 동일한 어휘가 반복되는 문제점이 생기기도함
    - Supervised : 문서전체를 집어넣고 요약문 비교한다.
    - Unsupervised : 딥러닝이 나오기전에 사용
> Page Rank(구글) 에서 파생된 text Rank : 중요한 문장에 대한 Score가 나오는 알고리즘
History
문서에서 주요한 문장만 잘 찾아서 그것을 추출만 하는 연구방식이 많이 진행됨
2015~ 부터 딥러닝 기반의 생성요약이 연구되기 시작함
Query Driven(Top-down) vs Text-Driven(Bottom-Up) Focus
Query-Driven : 특정한 주제를 하나 정해두고 문서에서 정보 추출을 하겠다.
Text-Driven : 문서하나가 주어지면 시스템이 알아서 요약문을 생성해주는 방법
- NLP vs IR
특정한 주제를 정해두고 문서로 부터 추출한다.
"테러라는 특정 주제라면 장소, 무기, ... 등을 정해두고 문서에서 뽑아낸다"
" 테러가 벌어졌다는 장소를 뽑아냄 "
그때 자연어처리 기술를 이용하거나
- NLP : 자연어처리로 처리되는 방법인가
    - 정해진 domain 에 대해서 꽤 높은 성능을 보인다.
    - 속도가 느리다.
    - open-domain으로 확장을 하려면 문제가 발생("테러", "재난" 등)
- IR : 정보검색 통계정보로만 이용한 방식
    - 중요한 문장이 뭔지 뽑아주기만 한다.
    - 문서에 대한 내용을 이해해서 만드는것이 아닌 통계정보를 이용하여 문장 Scoring으로 추출
    - 장점은 확장성이 높다.
    - NLP 방식에 비해서 퀄리티가 떨어진다.
## Extractive Summarization(과거방식)
10문장 중에서 정말 중요한 문장을 선택하는 방법
- Position 기반 : 문서의 앞 or 중간 or 끝 으로 추출
- 단어의 프리퀀시가 높은것을 기반으로 추출
    - 너무 높은 단어는 아니다 ( 'a', 'of' )
    - 중간쯤으로 많이 발생한 단어가 중요할 것이다
- 문장과 문장사이의 응집성을 수치화해서 중요한 문장을 찾아내겠다.
    - 단어의 상호참조 ( "문재인", "문대통령", "그" )
    - 중요한 문장은 다른 문장과 응집성을 가질 것이다.
- Discourse Structure of the text
    - 형 - 구 - 의 disocourse ( 여러 문장에 관해서)
- Template
- Page Rank - Text Rank(Graph-based methods)
# Abstractive Summarization
Copying Mechanism
생성을 분류를 통해서 하는것이 아니라 주어진 입력문서에서 내가 카피할 단어를 찾아서 그단어를 찍어주면 된다
추출요약은 언슈퍼바이즈 러닝이 가능했지만
생성요약은 NMT 모델을 사용하기때문에 학습 모델이 필요함, 학습데이터가 필요
학습데이터가 사용된다면 Domain의 영향을 받음
뉴스데이터만 학습된 모델은 뉴스 기사에만 적용을 해야지 다른 도메인에 사용하면 성능이 떨어진다.
## Neural Text Summarization
문서요약에 NMT 모델을 그대로 적용해보니 문제점이 발생함.(Seq2Seq)
- 여러 문장이 한꺼번에 들어가기 때문에 seq2Seq 사용했던 인코더쪽 문제가 발생하는데 그것을 해결하는방법(단어레벨의 Attention, 문장간의 Attention)
- Decoder 쪽도 Rnn 하나로 한문장을 만들었지만 여러 문장일수도 있으니 Decoder를 변경시켜서 여러문장을 생성할수있는 모델링을 하기도함
- 문서요약에서 반복문제를 어떻게 해결할것인가
- 기존에는 중요한 문장을 찾은뒤 그것중에 요약을 하였는데.
- 딥러닝으로 중요한 문장을 뽑고, 그걸 디코더에 넣어서 요약을 하는 모델도 있음
- 네트워크 문서 요약에 맞게끔 변경하겠끔 하는 시도도 있음
- Decoder 특성상 앞쪽이 틀리면 뒷부분이 틀리는데 이러한 뒷부분이 틀려도 잘 생성하도록 만드는 모델링 시도가 있음
> Beam search ?
> Diverse Beam decoding ?
## 논문들
### A Neural Attention Model for Abstractive Sentence Summarization
seq2seq 를 쓴 모델, Attention, Convolutional NN(주변에 있는 단어를 이용해서 사용하는 느낌)
### A Large Scale chinese Short Text Summarization Dataset
입력이 문서가 출력이 요약문이 나오는 논문, 워드를 분리시키는 과정이 필요(중국어이긴때문에)
Char 단위로하면 워드로 뽑아내는 과정에 나타나는 오류가 없기때문에 성능이 더 높다.
한국어도
### Abstractive Text Summarization using Sequence-to-sequence Rnns and Beyond
입력에 다양한 Feature 들을 집어넣음(TF-IDF)
Copying Mechanism
Switching Generator/Pointer model : Decoder에서 생성할때 Encoder 에서 들어온 입력들중에서 단어를 뽑아서 출력해주는 방식(copying)
Hierarchical Encoder model : 긴 문서 일경우(워드가 많을때) 문장의 대한 Attention을 구해보자
    - 워드 단위로 들어가는데 문장의 마지막 hidden state 값이 전체 문장을 의미하는 context Vector라 생각하고 두개의 Attention을 구함
하지만 Feature-rich-encoder 가 가장 성능이 좋았다.
### Pointer Network(goolge NIPS 15)
TSP 문제 : NP-hard
TSP 어느정도 해결된 예제들을 가지고 학습
인코더에서 똑같은데 디코더에서는 인코더 값을 카핑만 함(Attetion Mechism 을 사용하겠다는것과 같다.)
각각의 도시들에 대해서 좌표가 있는데 그것을 입력으로 넣어준다.
디코더에서는 Start 심볼에서 첫번째 노드를 찾아가고 그리고 그것을 디코더에서 넣어주고 다음 Point 위치를 찾는다.
### Copying Mechanism or CopyNet
기존의 Seq2Seq모델이 Copying Mechanism을 추가
인코더가 Bidirection을 써서 인코딩을 하고
디코더에서는 Start 에서 문장을 만들다가 사람이름 즉 Voca에 없는 단어는 인코더에서 카피를 해온다.
이때 생성을 할지 카피를 할지를 결정
Attention이 2개가 들어가는데 카핑을 위한 어텐션이 있다.
    - 일반적인 인코더쪽 Attention
    - Copying을 위한 Attention
기존에 voca_size에다가 인코더에 입력이 들어간 단어에 대해서만 어텐션 값이 더해지고 출력
voca에 안들어가는 것은 지브라라는 단어를 voca에 추가하고 그거의 확률을 올린다.
### Pointer-generator Model(ACL17)
Pgen 이라는것을 구해줘서 각 디코더 아웃풋과 인코더쪽 데이터에 가중치를 주고 그것을 최종 아웃풋 출력에 더한다.
## Temporal Attention
Attention이 한단어 집중되어서 다른 단어부분이 학습이 안되는 현상이 있는데 이걸 커버리지 메커니즘으로 해결되었는데
Attention이 한단어 반복되는 문제 : 걔에 관한 Context Vector에 들어가게 되고 같은 말을 반복하게 된다.(ABCABCABC...)
그래서 Attention을 관리해보자해서 나온것이 Temporal이랑 커버리지 mechanism
- 입력언어의 단어별로 누적 Attention을 구한다
- (새로운 Attention / 기존 Attetion) = score
- 기존 Attention 누적이 크면 내가 많이 본것이라 나누어 지게 되고, 적다면 0~1사이기 때문에 Attention이 조절된다.
## Coverage Mechnism
Temporal mechanism 과 비슷
### Modeling Coverage for Neural Machine Translation (ACL16)
u = 누적 스코어
인코더와 디코더의 히든스테이트를 사용하였는데 여기다가 커버리지 벡터값을 추가해서 MLP를 진행
누적벡터가 너무 커지지 않겠금 커버리지 로스라는걸 목적함수에 집어넣음
### Distraction based nn for modeling documents (IJACL)
### Absstrative document summarization with a graph-based attetnional neural model(ACL 17)
기본적인 Attention 을 사용한거에다가 Attention 메카니즘을 조금 바꿈, Graph based 로 더 중요한 Attention을 구함
Edge는 히든 스테이트간의 유사도로 구함
인코더 히든스테이트 각각 간의 유사도를 구함
중요하지 않은 논문
### Selective Encoding for Abstractive Sentence Summarization(ACL 17)
Attention을 사용하는데 Selective Gate Network를 사용해서 한번 더 Attention을 가공
인코더에 Hidden State를 한번 더 가공하자 (Selective Gate network ) s 값을 hidden에 추가하고 MLP 거쳐서 새로운 hidden나옴
히든 스테이트와 동일한 크기의 벡터가 나옴(기존에는 스칼라값이 나왔는데)
중요한 값이라면 벡터의 크기가 1로 나올텐데 그걸 기존 벡터와 Elemental Wise
인코더의 단어가 많아지만 어텐션값이 스무스하게 나온다. 그렇기 때문에 높은 어텐션을 가지는것은 더 크게 만들고 ,그렇지 않은 것은 더 낮춘다. 미리 한번 필터링 하자 중요한 애들만 위로 올라가서 다시 어텐션 스코어를 구하자
긴 문서에 대한 문제를 해결하는 논문
### A Deep Reinforced Model for Abstrative Summarization(arxiv17)
인트라 어텐션 - 셀프 어텐션이랑 비슷한 개념
만들었던것을 계속 반복하는(ABCABC...) 이런 문제가 계속 발생
앞에서 생성한 단어가 디코더의 RNN을 거쳐가면 뒤로 계속 전달이 되어야하는데 잘 안된거 아닐까?
그래서 디코더에서 지금까지 생성 한 단어들을 인트라-어텐션을 만들어서 다음 디코더 Time 에 전달
즉 디코더는 내가 앞에서 이런한 단어를 얘기했구나를 알고 다음 단어를 생성하기에 반복되는 현상이 없음
또다른 트릭은 강화학습을 도입 - 정답 차이값을 리워드로 도입
### Word-level Training: Scheduled Sampling
학습을 할때 잘못된 부분이 있지 않을까? 지금 까지 하던 학습방법을 개선해보자
Exposure bias 문제 : 인코더에는 문서 전체가 들어가고 디코더에는 정답 요약문이 들어가는데 디코더에서는 한칸씩 시프트한게 입력으로 들어가게 되서 실제로 돌다가 틀린답을 얻어내면 틀린답이 들어가게된다. 틀린단어가 들어갔을때는 어떻게 학습해야할지 생각을 안함
처음에는 정답을 넣다가 몇퍼센트 일정한 확률로 틀린답을 넣어주고 학습 시킨다.
Pdad가 확률
### Sequence-level Training: Reinforce
요약문들이 다 생성이되면 그때 학습을 시도
### Controlling Output Length in Neural Encoder-Decoders
문서의 길이를 조절하고싶음
디코더 쪽에서 문서의 길이를 조정, 빔서치 알고리즘을 조절해서,
디코더쪽에서 엔드심볼이 생성될때까지 단어가 생성되는데
FixLen : 빔서치할떄 엔드태그가 안나오게 한다. 원하는 길이까지 나오지 않으면 엔드태그 나오는 확률을 줄임
FixRng : 빔서치안에 후보들중에 지정된 길이 이외의 후보들은 제외시킨다.
LenInit : 히든스테이트에다가 지정된 길이정보를 집어넣어주면 히든을 타면서 디코더가 지정된 길이 맞춰 요약문을 생성